{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import re\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "from preproc import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from dPCA import dPCA\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "from itertools import combinations\n",
    "from scipy.special import factorial\n",
    "from scipy.stats import f_oneway\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import ttest_1samp\n",
    "from scipy.stats import mannwhitneyu\n",
    "from statsmodels.multivariate.manova import MANOVA\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.signal import savgol_filter\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.widgets import Slider\n",
    "from IPython.display import HTML\n",
    "import textwrap\n",
    "import pickle\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['font.size'] = 12\n",
    "day_list = ['20181105', '20181102', '20181101']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to save figures\n",
    "figsave_dir = 'figures'\n",
    "to_save = True\n",
    "save_name = 'July-2024'\n",
    "\n",
    "# Set name/annotation to use in plot titles\n",
    "popl_name = '364 cells'\n",
    "\n",
    "# Some common constants\n",
    "num_sess = len(day_list)\n",
    "num_goals = 6\n",
    "tbin_size = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common functions\n",
    "def group_by_trajectory(timeseries: np.array, trajectories: np.array) -> list:\n",
    "    # num_goals = 6\n",
    "    grouped = {(i+1, j+1): list() for j in range(num_goals) for i in range(num_goals) if i != j}\n",
    "    for idx, traj in enumerate(trajectories):\n",
    "        goal1, goal2 = int(traj[0]), int(traj[1])\n",
    "        if goal1 == 0:\n",
    "            continue\n",
    "        grouped[(goal1, goal2)].append(timeseries[idx])\n",
    "    return grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_cell_labels = list()\n",
    "# pattern = re.compile(r'/(\\d{8})/session\\d+/array0*\\d+/channel0*(\\d+)/cell0*(\\d+)')\n",
    "with open('data/cell_list_hm.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        line = line.strip().split('/')\n",
    "        good_cell_labels.append(f'{line[5]}ch{str(int(line[8][7:]))}c{str(int(line[9][4:]))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_cell_labels = list()\n",
    "with open('sigcells/place_cells.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        line = line.strip().split('/')\n",
    "        place_cell_labels.append(f'{line[5]}ch{str(int(line[8][7:]))}c{str(int(line[9][4:]))}')\n",
    "# View cells\n",
    "view_cell_labels = list()\n",
    "with open('sigcells/view_cells.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        line = line.strip().split('/')\n",
    "        view_cell_labels.append(f'{line[5]}ch{str(int(line[8][7:]))}c{str(int(line[9][4:]))}')\n",
    "# Merge and remove duplicates from both lists\n",
    "pv_cell_labels = list(set(place_cell_labels).union(set(view_cell_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cell_labels = list()\n",
    "all_net_information_gain = list()\n",
    "all_net_information_gain_scaled= list()\n",
    "\n",
    "for day in day_list:\n",
    "    with open(f'data/infogain/{day}_data.pkl', 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "        all_cell_labels.extend(data['cell_labels'])\n",
    "        all_net_information_gain.append(data['net_information_gain'])\n",
    "        all_net_information_gain_scaled.append(data['net_information_gain_scaled'])\n",
    "\n",
    "all_net_information_gain = np.vstack(all_net_information_gain)\n",
    "all_net_information_gain_scaled = np.vstack(all_net_information_gain_scaled)\n",
    "num_all_cells = len(all_cell_labels)\n",
    "for i, label in enumerate(all_cell_labels):\n",
    "    all_cell_labels[i] = all_cell_labels[i][4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseClassifier():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, X, y, thresh):\n",
    "        self.model.fit(X, y)\n",
    "        self.thresh = thresh\n",
    "\n",
    "    def set_thresh(self, thresh):\n",
    "        self.thresh = thresh\n",
    "\n",
    "    def predict(self, X):\n",
    "        classes = self.model.predict(X)\n",
    "        probs = self.model.predict_proba(X)\n",
    "        filt = np.where(np.max(probs, axis=1) < self.thresh)\n",
    "        classes[filt] = 0\n",
    "        return classes\n",
    "\n",
    "\n",
    "def train_test_split(X: np.array, y: np.array, split: tuple) -> tuple:\n",
    "    # itr: current iteration of k-fold validation\n",
    "    # way: value of k in k-fold validation\n",
    "    itr, way = split\n",
    "    X_train, X_test = list(), list()\n",
    "    y_train, y_test = list(), list()\n",
    "    for num, obs in enumerate(X):\n",
    "        if num % way == itr:\n",
    "            X_test.append(obs)\n",
    "            y_test.append(y[num])\n",
    "        else:\n",
    "            X_train.append(obs)\n",
    "            y_train.append(y[num])\n",
    "    # Return format: X_train, X_test, y_train, y_test   \n",
    "    return np.array(X_train), np.array(X_test), np.array(y_train), np.array(y_test)\n",
    "\n",
    "def confusion_matrix(y_pred: np.array, y_actl: np.array, num_classes: int) -> np.array:\n",
    "    res = np.zeros((num_classes, num_classes), dtype=int)\n",
    "    for pred, actl in zip(y_pred, y_actl):\n",
    "        pred, actl = int(pred - 1), int(actl - 1)\n",
    "        res[pred, actl] += 1\n",
    "    return res\n",
    "\n",
    "def prediction_accuracy(y_pred: np.array, y_actl: np.array) -> float:\n",
    "    count, total = 0, y_actl.shape[0]\n",
    "    for num, obs in enumerate(y_pred):\n",
    "        if obs == y_actl[num]:\n",
    "            count += 1\n",
    "    return count / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_spikerates_cue, all_spikerates_hints, all_spikerates_navend = list(), list(), list()\n",
    "all_goals_cue, all_goals_hints, all_goals_navend = list(), list(), list()\n",
    "all_trajectories_cue, all_trajectories_hints, all_trajectories_navend = list(), list(), list()\n",
    "\n",
    "'''\n",
    "for day in day_list:\n",
    "    with open(f'data/{day}_data.pkl', 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "        # Drop the first trial from each day's dataset\n",
    "        all_spikerates_cue.append(data['raw_data']['spikerates_cue'][1:])\n",
    "        all_spikerates_hints.append(data['raw_data']['spikerates_hints'][1:])\n",
    "        all_spikerates_navend.append(data['raw_data']['spikerates_navend'][1:])\n",
    "\n",
    "        all_goals_cue.append(data['raw_data']['goals_cue'][1:])\n",
    "        all_goals_hints.append(data['raw_data']['goals_hints'][1:])\n",
    "        all_goals_navend.append(data['raw_data']['goals_navend'][1:])\n",
    "\n",
    "        all_trajectories_cue.append(data['raw_data']['trajectories_cue'][1:])\n",
    "        all_trajectories_hints.append(data['raw_data']['trajectories_hints'][1:])\n",
    "        all_trajectories_navend.append(data['raw_data']['trajectories_navend'][1:])\n",
    "'''\n",
    "        \n",
    "for day in day_list:\n",
    "    with open(f'data/combined/{day}_data.pkl', 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "        # Drop the first trial from each day's dataset\n",
    "        all_spikerates_cue.append(data['cue_mean']['spikerates_cue'][1:])\n",
    "        all_spikerates_hints.append(data['hint_mean']['spikerates_hints'][1:])\n",
    "        all_spikerates_navend.append(data['navend_mean']['spikerates_navend'][1:])\n",
    "\n",
    "        all_goals_cue.append(data['cue_mean']['goals_cue'][1:])\n",
    "        all_goals_hints.append(data['hint_mean']['goals_hints'][1:])\n",
    "        all_goals_navend.append(data['navend_mean']['goals_navend'][1:])\n",
    "\n",
    "        all_trajectories_cue.append(data['cue_mean']['trajectories_cue'][1:])\n",
    "        all_trajectories_hints.append(data['hint_mean']['trajectories_hints'][1:])\n",
    "        all_trajectories_navend.append(data['navend_mean']['trajectories_navend'][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/jcheng/Documents/neural_decoding/Hippocampus_Decoding/dbscan_reclustering.ipynb Cell 10\u001b[0m line \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jcheng/Documents/neural_decoding/Hippocampus_Decoding/dbscan_reclustering.ipynb#X12sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mfor\u001b[39;00m traj, data \u001b[39min\u001b[39;00m combined_spikerates_cue_per_traj\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jcheng/Documents/neural_decoding/Hippocampus_Decoding/dbscan_reclustering.ipynb#X12sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     \u001b[39mfor\u001b[39;00m n, trial \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(data):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jcheng/Documents/neural_decoding/Hippocampus_Decoding/dbscan_reclustering.ipynb#X12sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m         data[n] \u001b[39m=\u001b[39m trial[:,cell_filter]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jcheng/Documents/neural_decoding/Hippocampus_Decoding/dbscan_reclustering.ipynb#X12sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mfor\u001b[39;00m traj, data \u001b[39min\u001b[39;00m combined_spikerates_hints_per_traj\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jcheng/Documents/neural_decoding/Hippocampus_Decoding/dbscan_reclustering.ipynb#X12sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     \u001b[39mfor\u001b[39;00m n, trial \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(data):\n",
      "\u001b[0;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "# Append cells from each session to form a pseudopopulation, append in chronological order on responses grouped by (start, end) goals\n",
    "# Cue phase spike rates\n",
    "all_spikerates_cue_per_traj = list()\n",
    "for i in range(num_sess):\n",
    "    all_spikerates_cue_per_traj.append(group_by_trajectory(all_spikerates_cue[i], all_trajectories_cue[i]))\n",
    "trajectories = list(all_spikerates_cue_per_traj[0].keys())\n",
    "combined_spikerates_cue_per_traj = {key: list() for key in trajectories}\n",
    "for traj in trajectories:\n",
    "    trials_data = [sess[traj] for sess in all_spikerates_cue_per_traj]\n",
    "    num_trials = min(map(len, trials_data))\n",
    "    for trial in range(num_trials):\n",
    "        combined_spikerates_cue_per_traj[traj].append(np.hstack([sess[trial] for sess in trials_data]))\n",
    "    combined_spikerates_cue_per_traj[traj] = np.vstack(combined_spikerates_cue_per_traj[traj])\n",
    "\n",
    "# Hint view spike rates\n",
    "all_spikerates_hints_per_traj = list()\n",
    "for i in range(num_sess):\n",
    "    all_spikerates_hints_per_traj.append(group_by_trajectory(all_spikerates_hints[i], all_trajectories_hints[i]))\n",
    "combined_spikerates_hints_per_traj = {key: list() for key in trajectories}\n",
    "for traj in trajectories:\n",
    "    trials_data = [sess[traj] for sess in all_spikerates_hints_per_traj]\n",
    "    num_trials = min(map(len, trials_data))\n",
    "    for trial in range(num_trials):\n",
    "        combined_spikerates_hints_per_traj[traj].append(np.hstack([sess[trial] for sess in trials_data]))\n",
    "    combined_spikerates_hints_per_traj[traj] = np.vstack(combined_spikerates_hints_per_traj[traj])\n",
    "\n",
    "# Nav end phase spike rates\n",
    "all_spikerates_navend_per_traj = list()\n",
    "for i in range(num_sess):\n",
    "    all_spikerates_navend_per_traj.append(group_by_trajectory(all_spikerates_navend[i], all_trajectories_navend[i]))\n",
    "combined_spikerates_navend_per_traj = {key: list() for key in trajectories}\n",
    "for traj in trajectories:\n",
    "    trials_data = [sess[traj] for sess in all_spikerates_navend_per_traj]\n",
    "    num_trials = min(map(len, trials_data))\n",
    "    for trial in range(num_trials):\n",
    "        combined_spikerates_navend_per_traj[traj].append(np.hstack([sess[trial] for sess in trials_data]))\n",
    "    combined_spikerates_navend_per_traj[traj] = np.vstack(combined_spikerates_navend_per_traj[traj])\n",
    "\n",
    "# Filter out cells not in good_cell_labels\n",
    "cell_filter = np.array([idx for idx, cell in enumerate(all_cell_labels) if cell in set(good_cell_labels)])\n",
    "\n",
    "for traj, data in combined_spikerates_cue_per_traj.items():\n",
    "    for n, trial in enumerate(data):\n",
    "        data[n] = trial[:,cell_filter]\n",
    "\n",
    "for traj, data in combined_spikerates_hints_per_traj.items():\n",
    "    for n, trial in enumerate(data):\n",
    "        data[n] = trial[:,cell_filter]\n",
    "\n",
    "for traj, data in combined_spikerates_navend_per_traj.items():\n",
    "    for n, trial in enumerate(data):\n",
    "        data[n] = trial[:,cell_filter]\n",
    "\n",
    "# Collect spike rate data into single arrays for each phase\n",
    "combined_spikerates_cue, combined_goals_cue = list(), list()\n",
    "for traj, obs in combined_spikerates_cue_per_traj.items():\n",
    "    combined_spikerates_cue.append(obs)\n",
    "    combined_goals_cue.extend(obs.shape[0]*[traj[1]])\n",
    "combined_spikerates_cue = np.vstack(combined_spikerates_cue)\n",
    "combined_goals_cue = np.array(combined_goals_cue)\n",
    "\n",
    "combined_spikerates_hints, combined_goals_hints = list(), list()\n",
    "for traj, obs in combined_spikerates_hints_per_traj.items():\n",
    "    combined_spikerates_hints.append(obs)\n",
    "    combined_goals_hints.extend(obs.shape[0]*[traj[1]])\n",
    "combined_spikerates_hints = np.vstack(combined_spikerates_hints)\n",
    "combined_goals_hints = np.array(combined_goals_hints)\n",
    "\n",
    "combined_spikerates_navend, combined_goals_navend = list(), list()\n",
    "for traj, obs in combined_spikerates_navend_per_traj.items():\n",
    "    combined_spikerates_navend.append(obs)\n",
    "    combined_goals_navend.extend(obs.shape[0]*[traj[1]])\n",
    "combined_spikerates_navend = np.vstack(combined_spikerates_navend)\n",
    "combined_goals_navend = np.array(combined_goals_navend)\n",
    "\n",
    "'''\n",
    "# Shuffle the order of the observations\n",
    "np.random.seed(0)\n",
    "shuffle_order = np.random.permutation(combined_goals_cue.shape[0])\n",
    "combined_spikerates_cue = combined_spikerates_cue[shuffle_order]\n",
    "combined_goals_cue = combined_goals_cue[shuffle_order]\n",
    "\n",
    "np.random.seed(0)\n",
    "shuffle_order = np.random.permutation(combined_goals_hints.shape[0])\n",
    "combined_spikerates_hints = combined_spikerates_hints[shuffle_order]\n",
    "combined_goals_hints = combined_goals_hints[shuffle_order]\n",
    "\n",
    "np.random.seed(0)\n",
    "shuffle_order = np.random.permutation(combined_goals_navend.shape[0])\n",
    "combined_spikerates_navend = combined_spikerates_navend[shuffle_order]\n",
    "combined_goals_navend = combined_goals_navend[shuffle_order]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up large memory variables\n",
    "del all_spikerates_cue\n",
    "del all_spikerates_hints\n",
    "del all_spikerates_navend\n",
    "del all_goals_cue\n",
    "del all_goals_hints\n",
    "del all_goals_navend\n",
    "del all_trajectories_cue\n",
    "del all_trajectories_hints\n",
    "del all_trajectories_navend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'combined_spikerates_cue' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/jcheng/Documents/neural_decoding/Hippocampus_Decoding/dbscan_reclustering.ipynb Cell 12\u001b[0m line \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jcheng/Documents/neural_decoding/Hippocampus_Decoding/dbscan_reclustering.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m ppop_models_goal \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jcheng/Documents/neural_decoding/Hippocampus_Decoding/dbscan_reclustering.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(k_fold):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jcheng/Documents/neural_decoding/Hippocampus_Decoding/dbscan_reclustering.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(combined_spikerates_cue, combined_goals_cue, (i, k_fold))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jcheng/Documents/neural_decoding/Hippocampus_Decoding/dbscan_reclustering.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     lda \u001b[39m=\u001b[39m LinearDiscriminantAnalysis()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jcheng/Documents/neural_decoding/Hippocampus_Decoding/dbscan_reclustering.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     lda\u001b[39m.\u001b[39mfit(X_train, y_train)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'combined_spikerates_cue' is not defined"
     ]
    }
   ],
   "source": [
    "# Build decoder for 6+1 classes\n",
    "k_fold = 10\n",
    "ppop_probmins_goal = np.zeros(k_fold)\n",
    "ppop_accuracy_goal_cue = np.zeros(k_fold)\n",
    "ppop_confusion_goal_cue = np.zeros((num_goals, num_goals))\n",
    "ppop_models_goal = list()\n",
    "for i in range(k_fold):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(combined_spikerates_cue, combined_goals_cue, (i, k_fold))\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    lda.fit(X_train, y_train)\n",
    "    y_pred = lda.predict(X_test)\n",
    "    y_pred_proba = lda.predict_proba(X_train)\n",
    "    y_pred_proba = np.max(y_pred_proba, axis=1)\n",
    "    ppop_probmins_goal[i] = np.min(y_pred_proba)\n",
    "    ppop_accuracy_goal_cue[i] = prediction_accuracy(y_pred, y_test)\n",
    "    ppop_confusion_goal_cue += confusion_matrix(y_pred, y_test, num_goals)\n",
    "    ppop_models_goal.append(lda)\n",
    "\n",
    "# Fit models using each fold of data\n",
    "ppop_confusion_goal_hints, ppop_confusion_goal_navend = np.zeros((num_goals+1, num_goals+1)), np.zeros((num_goals+1, num_goals+1))\n",
    "ppop_accuracy_goal_hints, ppop_accuracy_goal_navend = np.zeros(k_fold), np.zeros(k_fold)\n",
    "\n",
    "for i, model in enumerate(ppop_models_goal):\n",
    "    ppop_models_goal[i] = ResponseClassifier(model)\n",
    "    ppop_models_goal[i].set_thresh(ppop_probmins_goal[i])\n",
    "\n",
    "    # Prediction on hint views\n",
    "    hints_preds = model.predict(combined_spikerates_hints)\n",
    "    ppop_confusion_goal_hints += confusion_matrix(hints_preds, combined_goals_hints, num_goals+1)\n",
    "    ppop_accuracy_goal_hints[i] = prediction_accuracy(hints_preds, combined_goals_hints)\n",
    "\n",
    "    # Prediction on navend phases\n",
    "    navend_preds = model.predict(combined_spikerates_navend)\n",
    "    ppop_confusion_goal_navend += confusion_matrix(navend_preds, combined_goals_navend, num_goals+1)\n",
    "    ppop_accuracy_goal_navend[i] = prediction_accuracy(navend_preds, combined_goals_navend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteFrechet(object):\n",
    "    \"\"\"\n",
    "    Calculates the discrete Fréchet distance between two poly-lines using the\n",
    "    original recursive algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dist_func):\n",
    "        \"\"\"\n",
    "        Initializes the instance with a pairwise distance function.\n",
    "        :param dist_func: The distance function. It must accept two NumPy\n",
    "        arrays containing the point coordinates (x, y), (lat, long)\n",
    "        \"\"\"\n",
    "        self.dist_func = dist_func\n",
    "        self.ca = np.array([0.0])\n",
    "\n",
    "    def distance(self, p: np.ndarray, q: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the Fréchet distance between poly-lines p and q\n",
    "        This function implements the algorithm described by Eiter & Mannila\n",
    "        :param p: Poly-line p\n",
    "        :param q: Poly-line q\n",
    "        :return: Distance value\n",
    "        \"\"\"\n",
    "\n",
    "        def calculate(i: int, j: int) -> float:\n",
    "            \"\"\"\n",
    "            Calculates the distance between p[i] and q[i]\n",
    "            :param i: Index into poly-line p\n",
    "            :param j: Index into poly-line q\n",
    "            :return: Distance value\n",
    "            \"\"\"\n",
    "            if self.ca[i, j] > -1.0:\n",
    "                return self.ca[i, j]\n",
    "\n",
    "            d = self.dist_func(p[i], q[j])\n",
    "            if i == 0 and j == 0:\n",
    "                self.ca[i, j] = d\n",
    "            elif i > 0 and j == 0:\n",
    "                self.ca[i, j] = max(calculate(i-1, 0), d)\n",
    "            elif i == 0 and j > 0:\n",
    "                self.ca[i, j] = max(calculate(0, j-1), d)\n",
    "            elif i > 0 and j > 0:\n",
    "                self.ca[i, j] = max(min(calculate(i-1, j),\n",
    "                                        calculate(i-1, j-1),\n",
    "                                        calculate(i, j-1)), d)\n",
    "            else:\n",
    "                self.ca[i, j] = np.infty\n",
    "            return self.ca[i, j]\n",
    "\n",
    "        n_p = p.shape[0]\n",
    "        n_q = q.shape[0]\n",
    "        self.ca = np.zeros((n_p, n_q))\n",
    "        self.ca.fill(-1.0)\n",
    "        return calculate(n_p - 1, n_q - 1)\n",
    "    \n",
    "def mean_euclidean_distance(arr1, arr2):\n",
    "    distances = np.zeros_like(arr1)\n",
    "    for i in range(arr1.shape[0]):\n",
    "        distances[i] = euclidean_distance(arr1[i,:], arr2[i,:])\n",
    "    return np.mean(distances)\n",
    "\n",
    "def euclidean_distance(point1, point2):\n",
    "    return np.linalg.norm(point1 - point2)\n",
    "\n",
    "\n",
    "def bootstrapping(sess_data, num_trials):\n",
    "    # Bootstraps the session data in sess_data to meet the minimum number of required trials given by num_trials\n",
    "    # sess_data is given as a list, so output a list as well\n",
    "    # Set random seed for repeatable randomization\n",
    "    np.random.seed(0)\n",
    "    idxs = np.random.choice(np.arange(len(sess_data)), num_trials)\n",
    "    res = list()\n",
    "    for i in idxs:\n",
    "        res.append(sess_data[i])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_spikerates_cue = list()\n",
    "all_spikerates_iti_cue = list()\n",
    "# all_goals_cue = list()\n",
    "all_trajectories_cue = list()\n",
    "all_cell_labels = list()\n",
    "\n",
    "'''\n",
    "for day in day_list:\n",
    "    with open(f'data/pcaview/{day}_data.pkl', 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "        # Drop the first trial from each day's dataset\n",
    "        all_spikerates_cue.append(data['spikerates_cue'][1:])\n",
    "        all_goals_cue.append(data['goals_cue'][1:])\n",
    "        all_trajectories_cue.append(data['trajectories_cue'][1:])\n",
    "'''\n",
    "\n",
    "for day in day_list:\n",
    "    with open(f'data/combined/{day}_data.pkl', 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "        # Drop the first trial from each day's dataset\n",
    "        cue_data = data['cue_100ms']['spikerates_cue'][1:]\n",
    "        all_spikerates_cue.append(cue_data)\n",
    "        # Drop the last iti, tag itis to the next trial's cue phase\n",
    "        iti_data = data['iti_100ms']['spikerates_iti'][:-1]\n",
    "        # Drop the last navend, tag navends to the next trial's cue phase\n",
    "        navend_data = data['navend_100ms']['spikerates_navend'][:-1]\n",
    "        # Drop the first navst\n",
    "        navst_data = data['navst_100ms']['spikerates_navst'][1:]\n",
    "        for i in range(len(iti_data)):\n",
    "            iti_data[i] = np.concatenate([navend_data[i], iti_data[i], cue_data[i], navst_data[i]], axis=0)\n",
    "        all_spikerates_iti_cue.append(iti_data)\n",
    "        # all_goals_cue.append(data['cue_100ms']['goals_cue'][1:])\n",
    "        all_trajectories_cue.append(data['cue_100ms']['trajectories_cue'][1:])\n",
    "        all_cell_labels.extend(data['cell_labels'])\n",
    "\n",
    "# num_all_cells = len(all_cell_labels)\n",
    "num_all_cells = len(good_cell_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group trials within each session according to trajectory\n",
    "all_spikerates_cue_per_traj = list()\n",
    "for i in range(num_sess):\n",
    "    all_spikerates_cue_per_traj.append(group_by_trajectory(all_spikerates_cue[i], all_trajectories_cue[i]))\n",
    "\n",
    "all_spikerates_iti_cue_per_traj = list()\n",
    "for i in range(num_sess):\n",
    "    all_spikerates_iti_cue_per_traj.append(group_by_trajectory(all_spikerates_iti_cue[i], all_trajectories_cue[i]))\n",
    "\n",
    "trajectories = list(all_spikerates_cue_per_traj[0].keys())\n",
    "\n",
    "# Map out the number of trials per trajectory for each session\n",
    "num_trials_per_trajectory = {traj: [] for traj in trajectories}\n",
    "for sess in all_spikerates_cue_per_traj:\n",
    "    for traj in trajectories:\n",
    "        num_trials_per_trajectory[traj].append(len(sess[traj]))\n",
    "\n",
    "num_trials_per_trajectory_per_sess = np.zeros((num_sess, len(trajectories)))\n",
    "for i, traj in enumerate(trajectories):\n",
    "    num_trials_per_trajectory_per_sess[:,i] = num_trials_per_trajectory[traj]\n",
    "\n",
    "# Get target and lower bound (to drop sessions) of number of trials per trajectory\n",
    "target_num_trials = np.percentile(num_trials_per_trajectory_per_sess, 25, axis=0).astype(int)\n",
    "min_num_trials = (0.67 * target_num_trials).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "311"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(map(min, num_trials_per_trajectory.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Assemble pseudopopulation responses, grouped by trials of the same trajectory\n",
    "\n",
    "## Drop all trials beyond the least number of trials out of any session per trajectory\n",
    "# # Append cells from each session to form a pseudopopulation, append in chronological order on responses grouped by (start, end) goals\n",
    "# trajectories = list(all_spikerates_cue_per_traj[0].keys())\n",
    "# combined_spikerates_cue_per_traj = {key: list() for key in trajectories}\n",
    "# for traj in trajectories:\n",
    "#     trials_data = [sess[traj] for sess in all_spikerates_cue_per_traj]\n",
    "#     num_trials = min(map(len, trials_data))\n",
    "#     for trial in range(num_trials):\n",
    "#         for i, sess in enumerate(trials_data):\n",
    "#             if sess[trial].shape[0] > 10:\n",
    "#                 # Trim the length of the observation to 10x time bins (0 - 1000 ms)\n",
    "#                 trials_data[i][trial] = sess[trial][:10,:]\n",
    "#         combined_spikerates_cue_per_traj[traj].append(np.hstack([sess[trial] for sess in trials_data]))\n",
    "\n",
    "\n",
    "## Bootstrap sessions to some minimum number of trials per trajectory\n",
    "\n",
    "# Mark out sessions with at least 1 trajectory that falls below required min number of trials\n",
    "sessions_to_keep = set()\n",
    "for s in range(num_sess):\n",
    "    if np.all(num_trials_per_trajectory_per_sess[s,:] >= min_num_trials):\n",
    "        sessions_to_keep.add(s)\n",
    "\n",
    "# Update all_cell_labels after dropping sessions\n",
    "days_to_keep = set([day for d, day in enumerate(day_list) if d in sessions_to_keep])\n",
    "new_cell_labels = list()\n",
    "for cell in all_cell_labels:\n",
    "    if cell[:8] in days_to_keep:\n",
    "        new_cell_labels.append(cell)\n",
    "all_cell_labels = new_cell_labels\n",
    "\n",
    "# Append cells from each session to form a pseudopopulation, and append in chronological order on responses grouped by trajectories\n",
    "# But rather than dropping trials to the minimum number of observations for the given trajectory type, try to bootstrap up to the same average number of trials per trajectory\n",
    "combined_spikerates_cue_per_traj = {key: list() for key in trajectories}\n",
    "num_timebins = 10\n",
    "for t, traj in enumerate(trajectories):\n",
    "    # Set number of average trials per trajectory to meet\n",
    "    trials_data = [sess[traj] for s, sess in enumerate(all_spikerates_cue_per_traj) if s in sessions_to_keep]\n",
    "    for i, sess in enumerate(trials_data):\n",
    "        # num_trials_in_sess = len(sess)\n",
    "        if len(sess) < target_num_trials[t]:\n",
    "            # Bootstrap trials of this session if the number of obs is less than the required amount\n",
    "            trials_data[i] = bootstrapping(sess, target_num_trials[t])\n",
    "    for trial in range(target_num_trials[t]):\n",
    "        for i, sess in enumerate(trials_data):\n",
    "            if sess[trial].shape[0] > num_timebins:\n",
    "                # Trim the length of the observation to 10x time bins (0 - 1000 ms)\n",
    "                trials_data[i][trial] = sess[trial][:num_timebins,:]\n",
    "        combined_spikerates_cue_per_traj[traj].append(np.hstack([sess[trial] for sess in trials_data]))\n",
    "\n",
    "combined_spikerates_iti_cue_per_traj = {key: list() for key in trajectories}\n",
    "num_timebins_ext = 50\n",
    "for t, traj in enumerate(trajectories):\n",
    "    # Set number of average trials per trajectory to meet\n",
    "    trials_data = [sess[traj] for s, sess in enumerate(all_spikerates_iti_cue_per_traj) if s in sessions_to_keep]\n",
    "    for i, sess in enumerate(trials_data):\n",
    "        # num_trials_in_sess = len(sess)\n",
    "        if len(sess) < target_num_trials[t]:\n",
    "            # Bootstrap trials of this session if the number of obs is less than the required amount\n",
    "            trials_data[i] = bootstrapping(sess, target_num_trials[t])\n",
    "    for trial in range(target_num_trials[t]):\n",
    "        for i, sess in enumerate(trials_data):\n",
    "            if sess[trial].shape[0] > num_timebins_ext:\n",
    "                # Trim the length of the observation to __ time bins\n",
    "                trials_data[i][trial] = sess[trial][:num_timebins_ext,:]\n",
    "        combined_spikerates_iti_cue_per_traj[traj].append(np.hstack([sess[trial] for sess in trials_data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "338"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(map(len, combined_spikerates_cue_per_traj.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out cells not in good_cell_labels\n",
    "cell_filter = np.array([idx for idx, cell in enumerate(all_cell_labels) if cell in set(good_cell_labels)])\n",
    "for traj, data in combined_spikerates_cue_per_traj.items():\n",
    "    for n, trial in enumerate(data):\n",
    "        data[n] = trial[:,cell_filter]\n",
    "for traj, data in combined_spikerates_iti_cue_per_traj.items():\n",
    "    for n, trial in enumerate(data):\n",
    "        data[n] = trial[:,cell_filter]\n",
    "\n",
    "# Update num_all_cells and all_cell_labels to reflect number of cells in pseudopopulation\n",
    "num_all_cells = cell_filter.shape[0]\n",
    "all_cell_labels = [all_cell_labels[i] for i in cell_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine trials into a pseudosession and fit PCA to all trials\n",
    "combined_pcspikerates_cue = list()\n",
    "combined_trajectories_cue = list()\n",
    "for traj in trajectories:\n",
    "    combined_pcspikerates_cue.extend(combined_spikerates_cue_per_traj[traj])\n",
    "    combined_trajectories_cue.extend(len(combined_spikerates_cue_per_traj[traj]) * [traj])\n",
    "\n",
    "n_dims = 3\n",
    "pca = PCA(n_components=n_dims)\n",
    "combined_pcspikerates_cue = pca.fit_transform(np.vstack(combined_pcspikerates_cue))\n",
    "\n",
    "# Regroup into trajectories for plotting\n",
    "num_timebins = 10\n",
    "combined_pcspikerates_cue_per_traj = [[np.empty((0, num_timebins, n_dims)) for i in range(num_goals)] for j in range(num_goals)]\n",
    "for k, (i, j) in enumerate(combined_trajectories_cue):\n",
    "    i -= 1\n",
    "    j -= 1\n",
    "    combined_pcspikerates_cue_per_traj[i][j] = np.concatenate([combined_pcspikerates_cue_per_traj[i][j], combined_pcspikerates_cue[num_timebins*k:num_timebins*(k+1),:].reshape(-1, num_timebins, n_dims)])\n",
    "\n",
    "'''\n",
    "# Group cue phase responses according to trajectory, then group by same start goal and same end goal\n",
    "combined_spikerates_cue_start_goals, combined_spikerates_cue_end_goals = [list() for _ in range(num_goals)], [list() for _ in range(num_goals)]\n",
    "combined_start_goals_labels, combined_end_goals_labels = [list() for _ in range(num_goals)], [list() for _ in range(num_goals)]\n",
    "for traj, responses in combined_spikerates_cue_per_traj.items():\n",
    "    start_goal, end_goal = traj[0] - 1, traj[1] - 1\n",
    "    combined_spikerates_cue_start_goals[start_goal].extend(responses)\n",
    "    combined_start_goals_labels[start_goal].extend(len(responses) * [end_goal + 1])\n",
    "    combined_spikerates_cue_end_goals[end_goal].extend(responses)\n",
    "    combined_end_goals_labels[end_goal].extend(len(responses) * [start_goal + 1])\n",
    "\n",
    "# Fit PCA within each group of same starting/same ending goal\n",
    "pca = PCA(n_components=3)\n",
    "combined_pcspikerates_cue_start_goals, combined_pcspikerates_cue_end_goals = list(), list()\n",
    "for i in range(num_goals):\n",
    "    pc_start_goals = pca.fit_transform(np.vstack(combined_spikerates_cue_start_goals[i]))\n",
    "    pc_end_goals = pca.fit_transform(np.vstack(combined_spikerates_cue_end_goals[i]))\n",
    "    combined_pcspikerates_cue_start_goals.append(np.array([pc_start_goals[10*j:10*(j+1),:] for j in range(pc_start_goals.shape[0]//10)]))\n",
    "    combined_pcspikerates_cue_end_goals.append(np.array([pc_end_goals[10*j:10*(j+1),:] for j in range(pc_end_goals.shape[0]//10)]))\n",
    "\n",
    "# Regroup into trajectories for plotting\n",
    "combined_pcspikerates_cue_per_traj = [[np.empty((0, 10, 3)) for i in range(num_goals)] for j in range(num_goals)]\n",
    "for i in range(num_goals):\n",
    "    for j, data in enumerate(combined_pcspikerates_cue_start_goals[i]):\n",
    "        k = combined_start_goals_labels[i][j] - 1\n",
    "        combined_pcspikerates_cue_per_traj[i][k] = np.concatenate([combined_pcspikerates_cue_per_traj[i][k], data.reshape(-1, 10, 3)], axis=0)\n",
    "'''\n",
    "\n",
    "# Plot the average response per trajectory\n",
    "combined_pcspikerates_cue_per_traj_avg = [[np.empty((0, num_timebins, n_dims)) for i in range(num_goals)] for j in range(num_goals)]\n",
    "for i in range(num_goals):\n",
    "    for j, data in enumerate(combined_pcspikerates_cue_per_traj[i]):\n",
    "        if i == j:\n",
    "            continue\n",
    "        combined_pcspikerates_cue_per_traj_avg[i][j] = np.concatenate([combined_pcspikerates_cue_per_traj_avg[i][j], np.mean(data, axis=0).reshape(1, num_timebins, n_dims)], axis=0)\n",
    "\n",
    "'''\n",
    "# Subtract the first frame from all subsequent frames to get the displacement from start\n",
    "combined_pcspikerates_cue_per_traj_ref = [[combined_pcspikerates_cue_per_traj_avg[i][j][:,0,:] for j in range(num_goals)] for i in range(num_goals)]\n",
    "combined_pcspikerates_cue_per_traj_start = [[combined_pcspikerates_cue_per_traj_avg[i][j].copy() for j in range(num_goals)] for i in range(num_goals)]\n",
    "for i in range(num_goals):\n",
    "    for j in range(num_goals):\n",
    "        if i == j:\n",
    "            continue\n",
    "        combined_pcspikerates_cue_per_traj_start[i][j] -= combined_pcspikerates_cue_per_traj_ref[i][j]\n",
    "\n",
    "# Subtract the last frame from all previous frames to get the displacement from end\n",
    "combined_pcspikerates_cue_per_traj_ref = [[combined_pcspikerates_cue_per_traj_avg[i][j][:,-1,:] for j in range(num_goals)] for i in range(num_goals)]\n",
    "combined_pcspikerates_cue_per_traj_end = [[combined_pcspikerates_cue_per_traj_avg[i][j].copy() for j in range(num_goals)] for i in range(num_goals)]\n",
    "for i in range(num_goals):\n",
    "    for j in range(num_goals):\n",
    "        if i == j:\n",
    "            continue\n",
    "        combined_pcspikerates_cue_per_traj_end[i][j] -= combined_pcspikerates_cue_per_traj_ref[i][j]\n",
    "'''\n",
    "\n",
    "# Calculate Frechet distances between each group, Euclidean distances between start points across groups,\n",
    "# between end points across groups, and between start and end within group\n",
    "frechet = DiscreteFrechet(euclidean_distance)\n",
    "combined_pcspikerates_cue_per_traj_frechdist = [[dict(), dict()] for _ in range(num_goals)]\n",
    "combined_pcspikerates_cue_per_traj_meaneucdist = [[dict(), dict()] for _ in range(num_goals)]\n",
    "combined_pcspikerates_cue_per_traj_startdist = [[dict(), dict()] for _ in range(num_goals)]\n",
    "combined_pcspikerates_cue_per_traj_enddist = [[dict(), dict()] for _ in range(num_goals)]\n",
    "combined_pcspikerates_cue_per_traj_startenddist = [[dict(), dict()] for _ in range(num_goals)]\n",
    "for i in range(num_goals):\n",
    "    traj_pairs = list(range(num_goals))\n",
    "    traj_pairs.remove(i)\n",
    "    for j in traj_pairs:\n",
    "        combined_pcspikerates_cue_per_traj_startenddist[i][0][j] = euclidean_distance(combined_pcspikerates_cue_per_traj_avg[i][j][0,0,:], combined_pcspikerates_cue_per_traj_avg[i][j][0,-1,:])\n",
    "        combined_pcspikerates_cue_per_traj_startenddist[i][1][j] = euclidean_distance(combined_pcspikerates_cue_per_traj_avg[j][i][0,0,:], combined_pcspikerates_cue_per_traj_avg[j][i][0,-1,:])\n",
    "\n",
    "    traj_pairs = list(combinations(traj_pairs, 2))\n",
    "    for (j1, j2) in traj_pairs:\n",
    "        combined_pcspikerates_cue_per_traj_frechdist[i][0][(j1, j2)] = frechet.distance(combined_pcspikerates_cue_per_traj_avg[i][j1][0,:,:], combined_pcspikerates_cue_per_traj_avg[i][j2][0,:,:])\n",
    "        combined_pcspikerates_cue_per_traj_frechdist[i][1][(j1, j2)] = frechet.distance(combined_pcspikerates_cue_per_traj_avg[j1][i][0,:,:], combined_pcspikerates_cue_per_traj_avg[j2][i][0,:,:])\n",
    "        combined_pcspikerates_cue_per_traj_meaneucdist[i][0][(j1, j2)] = mean_euclidean_distance(combined_pcspikerates_cue_per_traj_avg[i][j1][0,:,:], combined_pcspikerates_cue_per_traj_avg[i][j2][0,:,:])\n",
    "        combined_pcspikerates_cue_per_traj_meaneucdist[i][1][(j1, j2)] = mean_euclidean_distance(combined_pcspikerates_cue_per_traj_avg[j1][i][0,:,:], combined_pcspikerates_cue_per_traj_avg[j2][i][0,:,:])\n",
    "\n",
    "        combined_pcspikerates_cue_per_traj_startdist[i][0][(j1, j2)] = euclidean_distance(combined_pcspikerates_cue_per_traj_avg[i][j1][0,0,:], combined_pcspikerates_cue_per_traj_avg[i][j2][0,0,:])\n",
    "        combined_pcspikerates_cue_per_traj_startdist[i][1][(j1, j2)] = euclidean_distance(combined_pcspikerates_cue_per_traj_avg[j1][i][0,0,:], combined_pcspikerates_cue_per_traj_avg[j2][i][0,0,:])\n",
    "        combined_pcspikerates_cue_per_traj_enddist[i][0][(j1, j2)] = euclidean_distance(combined_pcspikerates_cue_per_traj_avg[i][j1][0,-1,:], combined_pcspikerates_cue_per_traj_avg[i][j2][0,-1,:])\n",
    "        combined_pcspikerates_cue_per_traj_enddist[i][1][(j1, j2)] = euclidean_distance(combined_pcspikerates_cue_per_traj_avg[j1][i][0,-1,:], combined_pcspikerates_cue_per_traj_avg[j2][i][0,-1,:])\n",
    "\n",
    "\n",
    "# Calculate separation between clusters per (100 ms) timestep in cue phase\n",
    "combined_pcspikerates_cue_per_traj_vrc = [[np.zeros(num_timebins), np.zeros(num_timebins)] for _ in range(num_goals)]\n",
    "combined_pcspikerates_cue_per_traj_dbi = [[np.zeros(num_timebins), np.zeros(num_timebins)] for _ in range(num_goals)]\n",
    "for i in range(num_goals):\n",
    "    pcspikerates_start, pcspikerates_end = np.zeros((0, num_timebins, n_dims)), np.zeros((0, num_timebins, n_dims))\n",
    "    goals_start, goals_end = list(), list()\n",
    "    for j in range(num_goals):\n",
    "        if i == j:\n",
    "            continue\n",
    "        pcspikerates_start = np.concatenate([pcspikerates_start, combined_pcspikerates_cue_per_traj[i][j]], axis=0)\n",
    "        pcspikerates_end = np.concatenate([pcspikerates_end, combined_pcspikerates_cue_per_traj[j][i]], axis=0)\n",
    "        goals_start.extend(combined_pcspikerates_cue_per_traj[i][j].shape[0] * [j])\n",
    "        goals_end.extend(combined_pcspikerates_cue_per_traj[j][i].shape[0] * [j])\n",
    "    for t in range(10):\n",
    "        combined_pcspikerates_cue_per_traj_vrc[i][0][t] = calinski_harabasz_score(pcspikerates_start[:,t,:], np.array(goals_start))\n",
    "        combined_pcspikerates_cue_per_traj_vrc[i][1][t] = calinski_harabasz_score(pcspikerates_end[:,t,:], np.array(goals_end))\n",
    "        combined_pcspikerates_cue_per_traj_dbi[i][0][t] = davies_bouldin_score(pcspikerates_start[:,t,:], np.array(goals_start))\n",
    "        combined_pcspikerates_cue_per_traj_dbi[i][1][t] = davies_bouldin_score(pcspikerates_end[:,t,:], np.array(goals_end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up large memory variables\n",
    "del all_spikerates_cue\n",
    "del all_spikerates_iti_cue\n",
    "# del all_goals_cue\n",
    "del all_trajectories_cue\n",
    "del all_spikerates_cue_per_traj\n",
    "del all_spikerates_iti_cue_per_traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jcheng/miniconda3/envs/decoding/lib/python3.10/site-packages/numpy/lib/function_base.py:2829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/Users/jcheng/miniconda3/envs/decoding/lib/python3.10/site-packages/numpy/lib/function_base.py:2830: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n"
     ]
    }
   ],
   "source": [
    "# Array axes: observation, timebin, cell\n",
    "combined_spikerates_iti_cue = np.empty((0, num_timebins_ext, num_all_cells))\n",
    "for (s, g) in trajectories:\n",
    "    combined_spikerates_iti_cue = np.concatenate([combined_spikerates_iti_cue, np.stack(combined_spikerates_iti_cue_per_traj[(s, g)])], axis=0)\n",
    "combined_trajectories_iti_cue = np.array(combined_trajectories_cue)\n",
    "\n",
    "# Compute correlation between cells\n",
    "cell_correlation = list()\n",
    "for timebin in range(num_timebins_ext):\n",
    "    cell_correlation.append(np.corrcoef(combined_spikerates_iti_cue[:,timebin,:], rowvar=False))\n",
    "cell_correlation = np.array(cell_correlation)\n",
    "\n",
    "# Set correlation matrix diagonal to zero\n",
    "for t in range(num_timebins_ext):\n",
    "    for c in range(num_all_cells):\n",
    "        cell_correlation[t,c,c] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.00000000e+00 -4.03948063e-02 -3.37814319e-04 ...  6.17163919e-02\n",
      "    1.50695094e-01 -1.27340582e-02]\n",
      "  [-4.03948063e-02  0.00000000e+00 -9.72349319e-03 ... -3.38628307e-02\n",
      "    6.23137418e-02 -9.41301605e-03]\n",
      "  [-3.37814319e-04 -9.72349319e-03  0.00000000e+00 ...  1.88603616e-02\n",
      "    4.13119170e-03 -3.23873767e-02]\n",
      "  ...\n",
      "  [ 6.17163919e-02 -3.38628307e-02  1.88603616e-02 ...  0.00000000e+00\n",
      "   -1.35810901e-02 -1.06749183e-02]\n",
      "  [ 1.50695094e-01  6.23137418e-02  4.13119170e-03 ... -1.35810901e-02\n",
      "    0.00000000e+00  6.55416977e-04]\n",
      "  [-1.27340582e-02 -9.41301605e-03 -3.23873767e-02 ... -1.06749183e-02\n",
      "    6.55416977e-04  0.00000000e+00]]\n",
      "\n",
      " [[ 0.00000000e+00 -4.09911594e-02  5.70814813e-02 ...  2.50769501e-03\n",
      "    2.99906583e-03 -2.22159734e-02]\n",
      "  [-4.09911594e-02  0.00000000e+00  1.91888041e-03 ... -3.31579860e-02\n",
      "   -6.97752140e-02  1.69640977e-01]\n",
      "  [ 5.70814813e-02  1.91888041e-03  0.00000000e+00 ...  2.08693968e-03\n",
      "   -3.54188555e-02 -5.75453040e-02]\n",
      "  ...\n",
      "  [ 2.50769501e-03 -3.31579860e-02  2.08693968e-03 ...  0.00000000e+00\n",
      "   -2.18063973e-03 -1.79706294e-02]\n",
      "  [ 2.99906583e-03 -6.97752140e-02 -3.54188555e-02 ... -2.18063973e-03\n",
      "    0.00000000e+00 -7.45454592e-02]\n",
      "  [-2.22159734e-02  1.69640977e-01 -5.75453040e-02 ... -1.79706294e-02\n",
      "   -7.45454592e-02  0.00000000e+00]]\n",
      "\n",
      " [[ 0.00000000e+00 -3.82476259e-02  4.17623766e-02 ... -3.68151385e-02\n",
      "   -6.96609637e-02 -1.89951267e-02]\n",
      "  [-3.82476259e-02  0.00000000e+00 -4.32859924e-02 ... -3.49480523e-02\n",
      "   -3.33697133e-02 -1.80317856e-02]\n",
      "  [ 4.17623766e-02 -4.32859924e-02  0.00000000e+00 ...  5.15273114e-02\n",
      "   -5.49258900e-02 -1.28265860e-02]\n",
      "  ...\n",
      "  [-3.68151385e-02 -3.49480523e-02  5.15273114e-02 ...  0.00000000e+00\n",
      "   -8.90088886e-02 -1.73564415e-02]\n",
      "  [-6.96609637e-02 -3.33697133e-02 -5.49258900e-02 ... -8.90088886e-02\n",
      "    0.00000000e+00 -5.16134493e-02]\n",
      "  [-1.89951267e-02 -1.80317856e-02 -1.28265860e-02 ... -1.73564415e-02\n",
      "   -5.16134493e-02  0.00000000e+00]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.00000000e+00 -2.56302433e-02  1.54503053e-01 ... -2.07837346e-16\n",
      "   -6.65399113e-02  8.88301269e-02]\n",
      "  [-2.56302433e-02  0.00000000e+00 -1.41509237e-02 ... -2.79299101e-02\n",
      "   -5.36698461e-02 -1.29774053e-02]\n",
      "  [ 1.54503053e-01 -1.41509237e-02  0.00000000e+00 ...  8.92004230e-03\n",
      "   -9.57003846e-03 -2.05504323e-02]\n",
      "  ...\n",
      "  [-2.07837346e-16 -2.79299101e-02  8.92004230e-03 ...  0.00000000e+00\n",
      "   -3.59031024e-02 -2.90401160e-02]\n",
      "  [-6.65399113e-02 -5.36698461e-02 -9.57003846e-03 ... -3.59031024e-02\n",
      "    0.00000000e+00  7.68684269e-02]\n",
      "  [ 8.88301269e-02 -1.29774053e-02 -2.05504323e-02 ... -2.90401160e-02\n",
      "    7.68684269e-02  0.00000000e+00]]\n",
      "\n",
      " [[ 0.00000000e+00 -3.00740742e-02 -3.45692763e-02 ... -4.73180237e-02\n",
      "   -2.30207895e-02 -4.75037114e-02]\n",
      "  [-3.00740742e-02  0.00000000e+00  1.36004586e-01 ... -2.52245701e-02\n",
      "   -1.90497628e-02  5.69780044e-02]\n",
      "  [-3.45692763e-02  1.36004586e-01  0.00000000e+00 ... -4.02636495e-02\n",
      "    2.12135246e-02  2.46822489e-02]\n",
      "  ...\n",
      "  [-4.73180237e-02 -2.52245701e-02 -4.02636495e-02 ...  0.00000000e+00\n",
      "    1.05503427e-02  5.02376377e-02]\n",
      "  [-2.30207895e-02 -1.90497628e-02  2.12135246e-02 ...  1.05503427e-02\n",
      "    0.00000000e+00 -6.45133550e-02]\n",
      "  [-4.75037114e-02  5.69780044e-02  2.46822489e-02 ...  5.02376377e-02\n",
      "   -6.45133550e-02  0.00000000e+00]]\n",
      "\n",
      " [[ 0.00000000e+00 -3.44845146e-02 -2.16830671e-02 ... -4.11065225e-02\n",
      "    9.13716923e-02 -1.20648282e-02]\n",
      "  [-3.44845146e-02  0.00000000e+00  4.16983869e-04 ... -2.88976594e-02\n",
      "   -3.62107220e-04 -8.48150794e-03]\n",
      "  [-2.16830671e-02  4.16983869e-04  0.00000000e+00 ... -3.96780861e-02\n",
      "   -9.36759764e-02 -2.45089983e-02]\n",
      "  ...\n",
      "  [-4.11065225e-02 -2.88976594e-02 -3.96780861e-02 ...  0.00000000e+00\n",
      "    1.22548825e-02 -1.01101988e-02]\n",
      "  [ 9.13716923e-02 -3.62107220e-04 -9.36759764e-02 ...  1.22548825e-02\n",
      "    0.00000000e+00 -4.29471065e-02]\n",
      "  [-1.20648282e-02 -8.48150794e-03 -2.45089983e-02 ... -1.01101988e-02\n",
      "   -4.29471065e-02  0.00000000e+00]]]\n"
     ]
    }
   ],
   "source": [
    "print(cell_correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBSCAN Cluster Assignments: [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "Number of clusters found: 0\n"
     ]
    }
   ],
   "source": [
    "data_DBSCAN = cell_correlation[timebin]\n",
    "data_standardized = StandardScaler().fit_transform(data_DBSCAN)\n",
    "eps = 0.55\n",
    "min_samples = 5\n",
    "dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "clusters_dbscan = dbscan.fit_predict(data_standardized)\n",
    "print(\"DBSCAN Cluster Assignments:\", clusters_dbscan)\n",
    "num_clusters_dbscan = len(set(clusters_dbscan)) - (1 if -1 in clusters_dbscan else 0)\n",
    "print(\"Number of clusters found:\", num_clusters_dbscan)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decoding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
